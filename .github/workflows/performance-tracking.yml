name: Performance Tracking

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run weekly performance benchmarks
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_category:
        description: 'Benchmark category to run'
        required: false
        default: 'basic'
        type: choice
        options:
          - basic
          - providers
          - concurrent
          - memory
          - realworld
          - all

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for version comparison

    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: '9.0.x'

    - name: Cache NuGet packages
      uses: actions/cache@v4
      with:
        path: ~/.nuget/packages
        key: ${{ runner.os }}-nuget-${{ hashFiles('**/*.csproj') }}
        restore-keys: |
          ${{ runner.os }}-nuget-

    - name: Restore dependencies
      run: dotnet restore

    - name: Build project
      run: dotnet build -c Release --no-restore

    - name: Setup benchmark environment
      run: |
        # Create performance data directory
        mkdir -p .performance-data

        # Get version information
        echo "VERSION=$(git describe --tags --always)" >> $GITHUB_ENV
        echo "COMMIT_SHA=$(git rev-parse HEAD)" >> $GITHUB_ENV
        echo "BRANCH_NAME=${GITHUB_REF#refs/heads/}" >> $GITHUB_ENV
        echo "TIMESTAMP=$(date -u '+%Y%m%d_%H%M%S')" >> $GITHUB_ENV

    - name: Run Redis for provider benchmarks
      if: ${{ github.event.inputs.benchmark_category == 'providers' || github.event.inputs.benchmark_category == 'all' || github.event_name == 'schedule' }}
      run: |
        docker run -d -p 6379:6379 redis:alpine
        sleep 5

    - name: Run performance benchmarks
      run: |
        cd MethodCache.Benchmarks

        CATEGORY="${{ github.event.inputs.benchmark_category || 'basic' }}"
        if [ "${{ github.event_name }}" == "schedule" ]; then
          CATEGORY="all"
        fi

        # Run benchmarks with optimized settings for CI
        ./run-benchmarks.sh $CATEGORY \
          --format json \
          --warmup 2 \
          --iterations 3 \
          --config Release

    - name: Process benchmark results
      run: |
        cd MethodCache.Benchmarks

        # Create results processor script
        cat > process-results.py << 'EOF'
        import json
        import os
        import sys
        from datetime import datetime

        def process_benchmark_results():
            version = os.environ.get('VERSION', 'unknown')
            commit_sha = os.environ.get('COMMIT_SHA', 'unknown')
            branch = os.environ.get('BRANCH_NAME', 'unknown')
            timestamp = datetime.utcnow().isoformat()

            # Find JSON results
            results_files = []
            for root, dirs, files in os.walk('BenchmarkDotNet.Artifacts'):
                for file in files:
                    if file.endswith('-report-full.json'):
                        results_files.append(os.path.join(root, file))

            if not results_files:
                print("No benchmark results found")
                return

            # Process each results file
            processed_results = {
                'metadata': {
                    'version': version,
                    'commit': commit_sha,
                    'branch': branch,
                    'timestamp': timestamp,
                    'environment': {
                        'os': 'ubuntu-latest',
                        'dotnet': '9.0.x'
                    }
                },
                'benchmarks': []
            }

            for file_path in results_files:
                try:
                    with open(file_path, 'r') as f:
                        data = json.load(f)

                    for benchmark in data.get('Benchmarks', []):
                        result = {
                            'name': benchmark.get('DisplayInfo', ''),
                            'method': benchmark.get('Method', ''),
                            'parameters': benchmark.get('Parameters', {}),
                            'statistics': {
                                'mean': benchmark.get('Statistics', {}).get('Mean', 0),
                                'error': benchmark.get('Statistics', {}).get('Error', 0),
                                'stdDev': benchmark.get('Statistics', {}).get('StandardDeviation', 0),
                                'median': benchmark.get('Statistics', {}).get('Median', 0),
                                'min': benchmark.get('Statistics', {}).get('Min', 0),
                                'max': benchmark.get('Statistics', {}).get('Max', 0)
                            },
                            'memory': {
                                'allocated': benchmark.get('Memory', {}).get('BytesAllocatedPerOperation', 0),
                                'gen0': benchmark.get('Memory', {}).get('Gen0CollectionsPerOperation', 0),
                                'gen1': benchmark.get('Memory', {}).get('Gen1CollectionsPerOperation', 0),
                                'gen2': benchmark.get('Memory', {}).get('Gen2CollectionsPerOperation', 0)
                            }
                        }
                        processed_results['benchmarks'].append(result)

                except Exception as e:
                    print(f"Error processing {file_path}: {e}")

            # Save processed results
            output_file = f"../.performance-data/benchmark-{os.environ.get('TIMESTAMP', 'unknown')}.json"
            with open(output_file, 'w') as f:
                json.dump(processed_results, f, indent=2)

            print(f"Processed {len(processed_results['benchmarks'])} benchmark results")
            print(f"Results saved to {output_file}")

        if __name__ == '__main__':
            process_benchmark_results()
        EOF

        python3 process-results.py

    - name: Commit performance data
      if: github.ref == 'refs/heads/main'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"

        git add .performance-data/

        if git diff --staged --quiet; then
          echo "No performance data changes to commit"
        else
          git commit -m "ðŸ“Š Update performance data for ${{ env.VERSION }}"
          git push
        fi

    - name: Generate performance report
      run: |
        cd .performance-data

        # Create performance report generator
        cat > generate-report.py << 'EOF'
        import json
        import os
        import glob
        from datetime import datetime
        import statistics

        def generate_performance_report():
            # Load all benchmark data files
            data_files = sorted(glob.glob('benchmark-*.json'), reverse=True)

            if not data_files:
                print("No performance data found")
                return

            print(f"Found {len(data_files)} performance data files")

            # Generate summary report
            latest_file = data_files[0]
            with open(latest_file, 'r') as f:
                latest_data = json.load(f)

            # Create markdown report
            report = []
            report.append("# ðŸ“Š Performance Benchmark Results")
            report.append("")
            report.append(f"**Latest Version:** {latest_data['metadata']['version']}")
            report.append(f"**Last Updated:** {latest_data['metadata']['timestamp']}")
            report.append(f"**Commit:** {latest_data['metadata']['commit'][:8]}")
            report.append("")

            # Group benchmarks by method
            methods = {}
            for benchmark in latest_data['benchmarks']:
                method = benchmark['method']
                if method not in methods:
                    methods[method] = []
                methods[method].append(benchmark)

            # Generate performance table
            report.append("## ðŸš€ Latest Performance Results")
            report.append("")
            report.append("| Method | Data Size | Model Type | Mean (ns) | Allocated Memory |")
            report.append("|--------|-----------|------------|-----------|------------------|")

            for method, benchmarks in sorted(methods.items()):
                for benchmark in sorted(benchmarks, key=lambda x: (x['parameters'].get('DataSize', 0), x['parameters'].get('ModelType', ''))):
                    params = benchmark['parameters']
                    stats = benchmark['statistics']
                    memory = benchmark['memory']

                    data_size = params.get('DataSize', 'N/A')
                    model_type = params.get('ModelType', 'N/A')
                    mean_ns = f"{stats['mean']:.2f}" if stats['mean'] > 0 else "N/A"
                    allocated = f"{memory['allocated']:,} B" if memory['allocated'] > 0 else "N/A"

                    report.append(f"| {method} | {data_size} | {model_type} | {mean_ns} | {allocated} |")

            report.append("")

            # Performance trends if we have historical data
            if len(data_files) > 1:
                report.append("## ðŸ“ˆ Performance Trends")
                report.append("")

                # Analyze trends for key benchmarks
                trend_methods = ['CacheHit', 'CacheMiss', 'NoCaching']

                for method in trend_methods:
                    report.append(f"### {method} Performance Over Time")
                    report.append("")
                    report.append("| Version | Date | Mean (ns) | Memory (B) | Change |")
                    report.append("|---------|------|-----------|------------|--------|")

                    previous_mean = None
                    for data_file in reversed(data_files[-10:]):  # Last 10 versions
                        try:
                            with open(data_file, 'r') as f:
                                data = json.load(f)

                            # Find benchmark for this method
                            method_benchmarks = [b for b in data['benchmarks']
                                               if b['method'] == method and
                                               b['parameters'].get('DataSize') == 1 and
                                               b['parameters'].get('ModelType') == 'Small']

                            if method_benchmarks:
                                benchmark = method_benchmarks[0]
                                mean = benchmark['statistics']['mean']
                                memory = benchmark['memory']['allocated']
                                version = data['metadata']['version']
                                date = data['metadata']['timestamp'][:10]

                                change = ""
                                if previous_mean is not None and mean > 0:
                                    change_pct = ((mean - previous_mean) / previous_mean) * 100
                                    if change_pct > 5:
                                        change = f"ðŸ“ˆ +{change_pct:.1f}%"
                                    elif change_pct < -5:
                                        change = f"ðŸ“‰ {change_pct:.1f}%"
                                    else:
                                        change = f"âž¡ï¸ {change_pct:+.1f}%"

                                report.append(f"| {version} | {date} | {mean:.2f} | {memory:,} | {change} |")
                                previous_mean = mean

                        except Exception as e:
                            print(f"Error processing {data_file}: {e}")

                    report.append("")

            # Save report
            with open('../PERFORMANCE.md', 'w') as f:
                f.write('\n'.join(report))

            print("Performance report generated: PERFORMANCE.md")

        if __name__ == '__main__':
            generate_performance_report()
        EOF

        python3 generate-report.py

    - name: Update README with performance data
      if: github.ref == 'refs/heads/main'
      run: |
        # Generate performance charts
        cd .performance-data
        python3 generate-charts.py --output-dir .

        # Update README performance section
        cd ..
        python3 .github/scripts/update-readme-performance.py

    - name: Commit README updates
      if: github.ref == 'refs/heads/main'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"

        git add README.md PERFORMANCE.md .performance-data/*.svg

        if git diff --staged --quiet; then
          echo "No README changes to commit"
        else
          git commit -m "ðŸ“Š Update README performance section for ${{ env.VERSION }}"
          git push
        fi

    - name: Upload performance artifacts
      uses: actions/upload-artifact@v4
      with:
        name: performance-results-${{ env.TIMESTAMP }}
        path: |
          .performance-data/
          PERFORMANCE.md
          MethodCache.Benchmarks/BenchmarkDotNet.Artifacts/
        retention-days: 90

    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          if (fs.existsSync('PERFORMANCE.md')) {
            const performance = fs.readFileSync('PERFORMANCE.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ðŸ“Š Performance Benchmark Results\n\n${performance}`
            });
          }

  performance-regression-check:
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request'

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Download performance artifacts
      uses: actions/download-artifact@v4
      with:
        name: performance-results-${{ env.TIMESTAMP || github.run_id }}

    - name: Check for performance regressions
      run: |
        cat > check-regression.py << 'EOF'
        import json
        import glob
        import sys

        def check_performance_regression():
            # Load latest benchmark data
            data_files = sorted(glob.glob('.performance-data/benchmark-*.json'), reverse=True)

            if len(data_files) < 2:
                print("Not enough data for regression analysis")
                return

            with open(data_files[0], 'r') as f:
                current_data = json.load(f)
            with open(data_files[1], 'r') as f:
                baseline_data = json.load(f)

            regressions = []
            improvements = []

            # Compare key benchmarks
            for current_benchmark in current_data['benchmarks']:
                method = current_benchmark['method']
                params = current_benchmark['parameters']
                current_mean = current_benchmark['statistics']['mean']

                # Find matching baseline benchmark
                for baseline_benchmark in baseline_data['benchmarks']:
                    if (baseline_benchmark['method'] == method and
                        baseline_benchmark['parameters'] == params):

                        baseline_mean = baseline_benchmark['statistics']['mean']

                        if baseline_mean > 0 and current_mean > 0:
                            change_pct = ((current_mean - baseline_mean) / baseline_mean) * 100

                            if change_pct > 10:  # 10% regression threshold
                                regressions.append({
                                    'method': method,
                                    'params': params,
                                    'change': change_pct,
                                    'current': current_mean,
                                    'baseline': baseline_mean
                                })
                            elif change_pct < -10:  # 10% improvement threshold
                                improvements.append({
                                    'method': method,
                                    'params': params,
                                    'change': change_pct,
                                    'current': current_mean,
                                    'baseline': baseline_mean
                                })
                        break

            # Report results
            if regressions:
                print("âš ï¸ Performance regressions detected!")
                for regression in regressions:
                    print(f"  - {regression['method']}: {regression['change']:+.1f}% slower")
                sys.exit(1)
            elif improvements:
                print("ðŸš€ Performance improvements detected!")
                for improvement in improvements:
                    print(f"  - {improvement['method']}: {improvement['change']:+.1f}% faster")
            else:
                print("âœ… No significant performance changes detected")

        if __name__ == '__main__':
            check_performance_regression()
        EOF

        python3 check-regression.py