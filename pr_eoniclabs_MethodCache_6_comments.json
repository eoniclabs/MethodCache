{
  "pr": {
    "owner": "eoniclabs",
    "repo": "MethodCache",
    "number": 6,
    "state": "open",
    "merged": false,
    "title": "Feature/performance improvements"
  },
  "summary": {
    "total_issue_comments": 2,
    "total_review_comments": 8,
    "total_all": 10
  },
  "issue_comments": [
    {
      "id": 3353540514,
      "author": "johmnym",
      "body": "/gemini review",
      "created_at": "2025-09-30T19:37:15Z",
      "html_url": "https://github.com/eoniclabs/MethodCache/pull/6#issuecomment-3353540514",
      "type": "issue_comment"
    },
    {
      "id": 3353540868,
      "author": "gemini-code-assist[bot]",
      "body": "## Summary of Changes\n\nHello @johmnym, I'm Gemini Code Assist[^1]! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed!\n\nThis pull request focuses on a comprehensive suite of performance improvements across the caching library. It primarily targets reducing allocations, enhancing concurrency for asynchronous operations, and optimizing key generation and in-memory cache management. The changes aim to make the caching mechanisms more efficient, scalable, and robust under various workloads.\n\n### Highlights\n\n* **Key Generation Enhancements**: The cache key generation logic across `MessagePackKeyGenerator`, `HttpCacheHandler`, and `VaryHeaderCacheKeyGenerator` has been refactored to use SHA256 hashing. This change improves key determinism, compactness, and security, moving away from simple string concatenation. Corresponding unit tests have been updated to reflect the new hashed key format.\n* **Asynchronous Operations Optimization**: Multiple asynchronous operations, particularly in `HybridCacheManager` and `HybridStorageManager`, have been optimized for parallel execution using `Task.WhenAll`. This includes cache invalidation, setting entries across multiple layers, and promoting entries. Similar optimizations were applied to Redis tag management and storage provider operations, reducing sequential `await` calls and improving overall concurrency.\n* **In-Memory Cache Performance**: The `AdvancedMemoryStorageProvider` received significant performance upgrades. Cache entry expiration and access tracking now use `Environment.TickCount64` for reduced allocations and improved speed. Eviction candidate selection (LFU, TTL, Random) has been optimized with sampling techniques to avoid expensive sorting on large collections. Memory usage estimation is more accurate, and tag index management has been refined to reduce lock contention.\n* **HTTP Caching Header Handling**: Improvements were made to how HTTP content headers are captured and restored in `HttpCacheEntry` to prevent issues with content stream consumption. The `NormalizeHeaderName` utility now uses `Span<char>` for reduced allocations and better performance when processing HTTP headers.\n* **Redis Provider Efficiency**: The Redis backplane now caches `JsonSerializerOptions` to minimize allocations during message serialization. Additionally, Redis Lua script arguments are now passed as pre-allocated arrays (`RedisValue[]`) instead of lists, further reducing allocations during script execution.\n\n\n\n\n\n\n\n<details>\n<summary><b>Using Gemini Code Assist</b></summary>\n<br>\n\nThe full guide for Gemini Code Assist can be found on our [documentation page](https://developers.google.com/gemini-code-assist/docs/review-github-code), here are some quick tips.\n\n<b>Invoking Gemini</b>\n\nYou can request assistance from Gemini at any point by creating a comment using either `/gemini <command>` or `@gemini-code-assist <command>`. Below is a summary of the supported commands on the current page.\n\nFeature | Command | Description\n--- | --- | ---\nCode Review | `/gemini review` | Performs a code review for the current pull request in its current state.\nPull Request Summary | `/gemini summary` | Provides a summary of the current pull request in its current state.\nComment | @gemini-code-assist | Responds in comments when explicitly tagged, both in pull request comments and review comments.\nHelp | `/gemini help` | Displays a list of available commands.\n\n<b>Customization</b>\n\nTo customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a `.gemini/` folder in the base of the repository. Detailed instructions can be found [here](https://developers.google.com/gemini-code-assist/docs/customize-gemini-behavior-github).\n\n<b>Limitations & Feedback</b>\n\nGemini Code Assist may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with :thumbsup: and :thumbsdown: on @gemini-code-assist comments. If you're interested in giving your feedback about your experience with Gemini Code Assist for Github and other Google products, sign up [here](https://google.qualtrics.com/jfe/form/SV_2cyuGuTWsEw84yG).\n\n<b>You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the [Gemini Code Assist IDE Extension](https://cloud.google.com/products/gemini/code-assist).</b>\n</details>\n\n\n\n\n[^1]: Review the [Privacy Notices](https://policies.google.com/privacy), [Generative AI Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy), [Terms of Service](https://policies.google.com/terms), and learn how to configure Gemini Code Assist in GitHub [here](https://developers.google.com/gemini-code-assist/docs/customize-gemini-behavior-github). Gemini can make mistakes, so double check it and [use code with caution](https://support.google.com/legal/answer/13505487).\n",
      "created_at": "2025-09-30T19:37:23Z",
      "html_url": "https://github.com/eoniclabs/MethodCache/pull/6#issuecomment-3353540868",
      "type": "issue_comment"
    }
  ],
  "review_comments": [
    {
      "id": 2392644863,
      "path": "MethodCache.Providers.Memory/Infrastructure/AdvancedMemoryStorageProvider.cs",
      "line": 87,
      "resolved": false,
      "outdated": false,
      "comments": [
        {
          "id": 2392644863,
          "author": "Copilot",
          "body": "ThreadLocal<Random> should be disposed properly. Consider implementing IDisposable or using a static ThreadLocal with proper cleanup in the Dispose method.",
          "created_at": "2025-09-30T19:38:19Z",
          "html_url": "https://github.com/eoniclabs/MethodCache/pull/6#discussion_r2392644863",
          "in_reply_to": null
        }
      ]
    },
    {
      "id": 2392644886,
      "path": "MethodCache.Providers.Memory/Infrastructure/AdvancedMemoryStorageProvider.cs",
      "line": 158,
      "resolved": false,
      "outdated": false,
      "comments": [
        {
          "id": 2392644886,
          "author": "Copilot",
          "body": "Using TotalMilliseconds for tick calculation is incorrect. Environment.TickCount64 returns milliseconds, but the conversion should use expiration.Ticks or multiply TotalMilliseconds by TimeSpan.TicksPerMillisecond for consistency with the tick-based system.",
          "created_at": "2025-09-30T19:38:20Z",
          "html_url": "https://github.com/eoniclabs/MethodCache/pull/6#discussion_r2392644886",
          "in_reply_to": null
        }
      ]
    },
    {
      "id": 2392644907,
      "path": "MethodCache.HttpCaching/Validation/VaryHeaderCacheKeyGenerator.cs",
      "line": 74,
      "resolved": false,
      "outdated": false,
      "comments": [
        {
          "id": 2392644907,
          "author": "Copilot",
          "body": "Using stackalloc for potentially large header names could cause stack overflow. Consider adding a size limit check and falling back to heap allocation for large headers (e.g., > 128 characters).\n```suggestion\n        // Use Span to avoid allocations for small header names; fall back to heap for large ones\n        const int StackAllocThreshold = 128;\n        Span<char> buffer = headerName.Length <= StackAllocThreshold\n            ? stackalloc char[headerName.Length]\n            : new char[headerName.Length];\n```",
          "created_at": "2025-09-30T19:38:20Z",
          "html_url": "https://github.com/eoniclabs/MethodCache/pull/6#discussion_r2392644907",
          "in_reply_to": null
        }
      ]
    },
    {
      "id": 2392644933,
      "path": "MethodCache.HttpCaching/HttpCacheHandler.cs",
      "line": 21,
      "resolved": false,
      "outdated": false,
      "comments": [
        {
          "id": 2392644933,
          "author": "Copilot",
          "body": "Converting from static field to instance field breaks shared state across handler instances. If this was intentional for isolation, it's correct, but if variant tracking should be global, this is a breaking change.\n```suggestion\n    private static readonly ConcurrentDictionary<string, ConcurrentQueue<string>> _variantIndex = new(StringComparer.Ordinal);\n```",
          "created_at": "2025-09-30T19:38:21Z",
          "html_url": "https://github.com/eoniclabs/MethodCache/pull/6#discussion_r2392644933",
          "in_reply_to": null
        }
      ]
    },
    {
      "id": 2392648878,
      "path": "MethodCache.Providers.Memory/Infrastructure/AdvancedMemoryStorageProvider.cs",
      "line": 556,
      "resolved": false,
      "outdated": false,
      "comments": [
        {
          "id": 2392648878,
          "author": "gemini-code-assist[bot]",
          "body": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThere is a critical race condition here. Another thread could add a key to the `keys` dictionary for this `tag` after the `keys.IsEmpty` check (line 553) but before `_tagToKeys.TryRemove` executes (line 555). This would lead to the `keys` dictionary being removed from `_tagToKeys` even though it's no longer empty, causing lost tag associations and data inconsistency. To fix this, you need to ensure the check and removal are atomic. The simplest way is to re-introduce a lock around this block, which was present before this refactoring.",
          "created_at": "2025-09-30T19:40:22Z",
          "html_url": "https://github.com/eoniclabs/MethodCache/pull/6#discussion_r2392648878",
          "in_reply_to": null
        }
      ]
    },
    {
      "id": 2392648885,
      "path": "MethodCache.Core/Storage/HybridStorageManager.cs",
      "line": 636,
      "resolved": false,
      "outdated": false,
      "comments": [
        {
          "id": 2392648885,
          "author": "gemini-code-assist[bot]",
          "body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis method iterates over all tracked keys and performs a linear search (`Array.IndexOf`) on the tags for each key. This can be inefficient if there are many tracked keys or many tags per key, with a complexity of O(M*N) where M is the number of keys and N is the average number of tags.\n\nTo improve performance, consider changing the `_trackedTags` dictionary to store a `HashSet<string>` for the tags instead of a `string[]`. This will make the lookup for a tag an O(1) operation.\n\nThis would require the following changes:\n1.  Change the field declaration: `private readonly ConcurrentDictionary<string, HashSet<string>> _trackedTags = new(StringComparer.Ordinal);`\n2.  Update `UpdateTrackedTags` to create a `HashSet`: `_trackedTags[key] = new HashSet<string>(tags, StringComparer.Ordinal);`\n3.  Update `GetTrackedTags` to return `IEnumerable<string>`.\n4.  Then, this method can be updated to use the more efficient `Contains` method.\n\nSeparately, the current implementation modifies the collection while iterating over it. While this is safe for `ConcurrentDictionary`, it's often clearer to first collect the keys to remove and then perform the removals, as shown in the suggestion.\n\n```c#\n    private void RemoveTrackedTagsByTag(string tag)\n    {\n        var keysToRemove = _trackedTags.Where(kvp => Array.IndexOf(kvp.Value, tag) >= 0)\n                                       .Select(kvp => kvp.Key)\n                                       .ToList();\n\n        foreach (var key in keysToRemove)\n        {\n            _trackedTags.TryRemove(key, out _);\n        }\n    }\n```",
          "created_at": "2025-09-30T19:40:22Z",
          "html_url": "https://github.com/eoniclabs/MethodCache/pull/6#discussion_r2392648885",
          "in_reply_to": null
        }
      ]
    },
    {
      "id": 2392650674,
      "path": "MethodCache.Providers.Memory/Infrastructure/AdvancedMemoryStorageProvider.cs",
      "line": 175,
      "resolved": false,
      "outdated": false,
      "comments": [
        {
          "id": 2392650674,
          "author": "chatgpt-codex-connector[bot]",
          "body": "**<sub><sub>![P1 Badge](https://img.shields.io/badge/P1-orange?style=flat)</sub></sub>  Avoid double-counting memory usage when setting entries**\n\nEach call to `SetAsync` now increments `_estimatedMemoryUsage` before calling `CheckAndEvictIfNeeded`, but `CheckAndEvictIfNeeded` still adds the same `estimatedSize` again. As a result every write adds the entry size twice (and only subtracts once on eviction), so the provider quickly believes it is over the memory limit and keeps evicting or reporting inflated usage even with few items. The previous revision only performed the increment inside `CheckAndEvictIfNeeded`; after moving the increment here the increment inside `CheckAndEvictIfNeeded` should have been removed.\n\nUseful? React with \ud83d\udc4d\u00a0/ \ud83d\udc4e.",
          "created_at": "2025-09-30T19:41:21Z",
          "html_url": "https://github.com/eoniclabs/MethodCache/pull/6#discussion_r2392650674",
          "in_reply_to": null
        }
      ]
    },
    {
      "id": 2392653060,
      "path": "MethodCache.Core/Storage/HybridStorageManager.cs",
      "line": 636,
      "resolved": false,
      "outdated": false,
      "comments": [
        {
          "id": 2392653060,
          "author": "gemini-code-assist[bot]",
          "body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe implementation of `RemoveTrackedTagsByTag` iterates through the entire `_trackedTags` dictionary, which has a time complexity of O(N*M) where N is the number of cached items and M is the average number of tags. For a large L1 cache, this linear scan could become a performance bottleneck during tag-based invalidation.\n\nTo optimize this, you could consider maintaining a reverse index (e.g., `ConcurrentDictionary<string, ConcurrentDictionary<string, byte>>`) to map tags to keys. This would change the complexity to be proportional to the number of keys associated with the tag, rather than all keys in the cache.",
          "created_at": "2025-09-30T19:42:38Z",
          "html_url": "https://github.com/eoniclabs/MethodCache/pull/6#discussion_r2392653060",
          "in_reply_to": null
        }
      ]
    }
  ]
}