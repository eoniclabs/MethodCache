# MethodCache Performance Comparison Strategy

## Overview

This framework provides **two distinct comparison approaches** to address the fundamental challenge of comparing source-generated code (MethodCache) with runtime-based caching libraries:

1. **Real Usage Comparison** (`RealMethodCacheComparison.cs`) - Shows actual performance as developers use each framework
2. **Adapter-Based Comparison** (`UnifiedCacheComparisonBenchmarks.cs`) - Normalized comparison through common interface

**Important**: These serve different purposes and will show different results. Understanding the difference is critical for proper interpretation.

## Supported Libraries

| Library | Version | Type | Stampede Protection |
|---------|---------|------|---------------------|
| **MethodCache** | Current | Source Generation | ✅ Yes |
| **FusionCache** | 2.0.0 | Runtime Proxy | ✅ Yes |
| **LazyCache** | 2.4.0 | Runtime Proxy | ✅ Yes |
| **EasyCaching** | 1.9.2 | AOP/Interceptor | ✅ Yes |
| **Microsoft.Extensions.Caching.Memory** | 9.0.9 | Direct API | ❌ No |

## The Adapter Problem

### Why Two Comparison Approaches?

MethodCache uses **compile-time source generation** to create optimized caching code, while other frameworks use **runtime abstractions** (proxies, interceptors, decorators). This fundamental difference creates a measurement challenge:

**Through Adapter Interface** (`ICacheAdapter`):
- MethodCache: ~700-1,000 ns (forced to use generic key generation)
- Baseline: ~50-100 ns
- Result: MethodCache looks 10-20x slower ❌

**Real Usage** (source-generated code):
- MethodCache: ~15-58 ns (optimized code path)
- Baseline: ~658 ns
- Result: MethodCache is 10-40x faster ✅

### The Root Cause: Key Generation Overhead

When MethodCache is used through an adapter, it must call `ICacheKeyGenerator.GenerateKey()` on every operation, which:
- Allocates object arrays for parameters
- Boxes value types
- Performs runtime type checking
- Serializes policy information
- Concatenates strings

**Real MethodCache code** (generated by source generator):
```csharp
// Specialized, optimized code for each method
public override SamplePayload GetData(string key)
{
    var cacheKey = $"GetData:{key}"; // Compile-time constant concatenation
    return _cache.Get<SamplePayload>(cacheKey) ?? ExecuteAndCache();
}
```

**Adapter-based code** (generic path):
```csharp
// Generic code forced through adapter
public bool TryGet<TValue>(string key, out TValue? value)
{
    // This generates the key at runtime with allocations
    var cacheKey = _keyGenerator.GenerateKey(
        key,
        new object[] { key },  // Allocation!
        _policy                 // Serialization!
    );
    // ... rest
}
```

The ~700ns difference is entirely from abstraction overhead that real MethodCache usage avoids.

## Architecture

### ICacheAdapter Interface

All libraries implement a common interface:

```csharp
public interface ICacheAdapter : IDisposable
{
    Task<TValue> GetOrSetAsync<TValue>(string key, Func<Task<TValue>> factory, TimeSpan duration);
    bool TryGet<TValue>(string key, out TValue? value);
    void Set<TValue>(string key, TValue value, TimeSpan duration);
    void Remove(string key);
    void Clear();
    CacheStatistics GetStatistics();
}
```

### Adapters

Each library has a dedicated adapter that translates between the common interface and the library's native API:

- `MethodCacheAdapter` - Uses ICacheManager directly
- `FusionCacheAdapter` - Wraps FusionCache
- `LazyCacheAdapter` - Wraps LazyCache/IAppCache
- `EasyCachingAdapter` - Wraps IEasyCachingProvider
- `MemoryCacheAdapter` - Wraps IMemoryCache

## Test Scenarios

### 1. Cache Hit Performance
**Tests:** Simple cache read operations
**Purpose:** Measure the overhead of cache retrieval
**Baseline:** Microsoft.Extensions.Caching.Memory

### 2. Cache Miss + Set
**Tests:** Factory execution and cache population
**Purpose:** Measure the cost of cache misses and storing values
**Baseline:** Microsoft.Extensions.Caching.Memory

### 3. Concurrent Access
**Tests:** Multiple threads accessing the same cache simultaneously
**Parameters:** 10, 100 threads
**Purpose:** Test thread-safety and contention handling

### 4. Cache Stampede Protection
**Tests:** 50 concurrent requests for the same missing key with expensive factory (50ms)
**Purpose:** Verify stampede protection mechanisms
**Expected:** Libraries with protection should call factory only once

## Running Comparisons

### Real Usage Comparison (Recommended)
Shows actual framework performance - **use these results for decision-making**:

```bash
dotnet run -c Release -- realcompare
```

**Results**: MethodCache 15-58 ns vs baseline 658 ns (10-40x faster)

### Adapter-Based Comparison
Normalized comparison through common interface - useful for understanding relative capabilities:

```bash
# Full comparison
dotnet run -c Release -- comparison

# Quick test (less accurate, faster)
BENCHMARK_QUICK=true dotnet run -c Release -- comparison

# Filter by category
dotnet run -c Release -- comparison --filter *CacheHit*
dotnet run -c Release -- comparison --filter *Stampede*
dotnet run -c Release -- comparison --filter *Concurrent*
```

**Note**: These results include ~700ns adapter overhead for MethodCache.

## Our Solution: Dual Comparison Strategy

We acknowledge both comparisons are valuable but serve different purposes:

### ✅ Use Real Usage Results When:
- **Choosing a caching framework** - You want actual performance numbers
- **Estimating performance impact** - You need realistic measurements
- **Reporting benchmarks publicly** - Show how the frameworks actually perform
- **Making architectural decisions** - Performance is a key factor

### ⚠️ Use Adapter-Based Results When:
- **Comparing normalized implementations** - Same workload through same interface
- **Testing specific scenarios** - Stampede prevention, concurrent access patterns
- **Understanding relative capabilities** - How frameworks handle edge cases
- **Academic interest** - Analyzing design tradeoffs

### ❌ Never:
- Use adapter-based results to claim MethodCache is slower than other frameworks
- Compare real-usage MethodCache numbers to adapter-based other framework numbers
- Mix results from both comparison types in the same analysis

## Understanding Results

### Key Metrics

- **Mean**: Average execution time
- **Error**: Half of 99.9% confidence interval
- **StdDev**: Standard deviation (consistency indicator)
- **Ratio**: Performance relative to baseline
- **Rank**: Relative ranking (1 = fastest)
- **Allocated**: Memory allocated per operation

### What to Look For

1. **Cache Hits**: Lower is better - measures cache overhead
2. **Miss + Set**: Includes factory execution - should be similar across libraries
3. **Concurrent**: Tests scalability - look for linear vs exponential scaling
4. **Stampede**: Count factory calls - should be 1 with protection, 50 without

## Adding New Libraries

To add a new caching library:

1. **Add NuGet Package** to `MethodCache.Benchmarks.csproj`

2. **Create Adapter** in `Comparison/Adapters/YourLibraryAdapter.cs`:
```csharp
public class YourLibraryAdapter : ICacheAdapter
{
    public string Name => "YourLibrary";

    public async Task<TValue> GetOrSetAsync<TValue>(...)
    {
        // Implement using library's API
    }

    // Implement other interface methods...
}
```

3. **Add to Benchmarks** in `UnifiedCacheComparisonBenchmarks.cs`:
```csharp
private ICacheAdapter _yourLibrary = null!;

[GlobalSetup]
public void GlobalSetup()
{
    _yourLibrary = new YourLibraryAdapter();
    // ...
}

[BenchmarkCategory("CacheHit"), Benchmark]
public bool YourLibrary_Hit()
{
    return _yourLibrary.TryGet<SamplePayload>(TestKey, out _);
}
```

4. **Build and Run**:
```bash
dotnet build MethodCache.Benchmarks -c Release
dotnet run --project MethodCache.Benchmarks -c Release -- comparison
```

## Interpretation Guide

### MethodCache Expected Advantages

1. **Compile-Time Code Generation**
   - Zero reflection overhead
   - Optimized method calls
   - Better JIT optimization opportunities

2. **Lightweight Runtime**
   - Minimal abstraction layers
   - Direct cache access
   - No proxy creation overhead

### Where Runtime Libraries May Excel

1. **Dynamic Scenarios**
   - No source code access needed
   - Works with interfaces/abstract classes
   - Runtime configuration flexibility

2. **Advanced Features**
   - Some libraries have richer feature sets
   - May include distributed caching out-of-box
   - Additional resilience patterns

## Example Results

### Real Usage Comparison Results
```
| Method                            | Mean        | Ratio | Rank | Allocated |
|---------------------------------- |------------:|------:|-----:|----------:|
| MethodCache_AdvancedMemory_Hit    |    15.38 ns |  0.03 |    1 |         - |
| MethodCache_CoreInMemory_Hit      |    58.10 ns |  0.12 |    2 |         - |
| Baseline_MemoryCache_Hit          |   658.40 ns |  1.32 |    3 |         - |
```
**Interpretation**: Real MethodCache is 10-40x faster than baseline

### Adapter-Based Comparison Results (Example)
```
| Method                | Mean       | Ratio | Rank | Allocated |
|---------------------- |-----------:|------:|-----:|----------:|
| MemoryCache_Hit       |    56.2 ns |  1.00 |    1 |         - |
| LazyCache_Hit         |    62.8 ns |  1.12 |    2 |         - |
| FusionCache_Hit       |    71.3 ns |  1.27 |    3 |         - |
| MethodCache_Hit       |   782.5 ns | 13.92 |    4 |      48 B |
| EasyCaching_Hit       |   845.1 ns | 15.04 |    5 |      64 B |
```
**Interpretation**: Adapter overhead makes MethodCache look slower—this is NOT real performance

## Contributing

When adding new comparison scenarios:

1. **Keep it Fair**: All libraries should do the same work
2. **Measure What Matters**: Focus on real-world patterns
3. **Document Assumptions**: Explain what each test validates
4. **Consider Context**: Some libraries optimize for different scenarios

## References

- **FusionCache Benchmarks**: `/Users/johan/dev/FusionCache/benchmarks`
- **MethodCache Core**: `../MethodCache.Core`
- **Benchmark Best Practices**: https://benchmarkdotnet.org/articles/guides/good-practices.html
